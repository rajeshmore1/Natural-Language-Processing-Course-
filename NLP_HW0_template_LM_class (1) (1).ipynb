{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pvaeDpH4MEc"
   },
   "source": [
    "# Advanced NLP HW0\n",
    "\n",
    "Before starting the task please read thoroughly these chapters of Speech and Language Processing by Daniel Jurafsky & James H. Martin:\n",
    "\n",
    "•\tN-gram language models: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "•\tNeural language models: https://web.stanford.edu/~jurafsky/slp3/7.pdf \n",
    "\n",
    "In this task you will be asked to implement the models described there.\n",
    "\n",
    "Build a text generator based on n-gram language model and neural language model.\n",
    "1.\tFind a corpus (e.g. http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt ), but you are free to use anything else of your interest\n",
    "2.\tPreprocess it if necessary (we suggest using nltk for that)\n",
    "3.\tBuild an n-gram model\n",
    "4.\tTry out different values of n, calculate perplexity on a held-out set\n",
    "5.\tBuild a simple neural network model for text generation (start from a feed-forward net for example). We suggest using tensorflow + keras for this task\n",
    "\n",
    "Criteria:\n",
    "1.\tData is split into train / validation / test, motivation for the split method is given\n",
    "2.\tN-gram model is implemented\n",
    "  *\tUnknown words are handled\n",
    "  * Add-k Smoothing is implemented\n",
    "3.\tNeural network for text generation is implemented\n",
    "4.\tPerplexity is calculated for both models\n",
    "5.\tExamples of texts generated with different models are present and compared\n",
    "6.\tOptional: Try both character-based and word-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Union, Tuple, List\n",
    "import random\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7FEwRuO6og0"
   },
   "source": [
    "## Custom ngram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCcHNlMEFaZP"
   },
   "source": [
    "Base class for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Aki99FguaeN9"
   },
   "outputs": [],
   "source": [
    "class BaseLM:\n",
    "    def _check_fit(func):\n",
    "        \"\"\"\n",
    "        A helper decorator that ensures that the LM was fit on vocab.\n",
    "        \"\"\"\n",
    "        @wraps(func)\n",
    "        def wrapper(self,*args,**kwargs):\n",
    "            if not self.is_fitted:\n",
    "                raise AttributeError(f\"Fit model before call {func.__name__} method\")\n",
    "            return func(self, *args,**kwargs)\n",
    "        return wrapper\n",
    "\n",
    "    def __init__(self, \n",
    "                 n: int, \n",
    "                 vocab: Iterable[str] = None, \n",
    "                 unk_label: str = \"<UNK>\"\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Language model constructor\n",
    "        n -- n-gram size\n",
    "        vocab -- optional fixed vocabulary for the model\n",
    "        unk_label -- special token that stands in for so-called \"unknown\" items\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self._vocab = vocab if vocab else None\n",
    "        self.unk_label = unk_label\n",
    "  \n",
    "    def _lookup(self, \n",
    "                words: Union[str, Iterable[str]]\n",
    "               ) -> Union[str, Tuple[str]]:\n",
    "        \"\"\"\n",
    "        Look ups words in the vocabulary\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @_check_fit\n",
    "    def prob(self, \n",
    "             word: str, \n",
    "             context: Tuple[str] = None\n",
    "            ) -> float:\n",
    "        \"\"\"This method returns probability of a word with given context: P(w_t | w_{t - 1}...w_{t - n + 1})\n",
    "\n",
    "        For example:\n",
    "        >>> lm.prob('hello', context=('world',))\n",
    "        0.99988\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def prob_with_smoothing(self, \n",
    "                            word: str, \n",
    "                            context: Tuple[str] = None, \n",
    "                            alpha: float = 1.0\n",
    "                            ) -> float:\n",
    "        \"\"\"Proabaility with Additive smoothing\n",
    "\n",
    "        see: https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "        where:\n",
    "        x - count of word in context\n",
    "        N - total\n",
    "        d - wocab size\n",
    "        a - alpha\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @_check_fit\n",
    "    def generate(self, \n",
    "                 text_length: int, \n",
    "                 text_seed: Iterable[str] = None,\n",
    "                 random_seed: Union[int,random.Random] = 42,\n",
    "                 prob_method = str\n",
    "                 ) -> List[str]:\n",
    "        \"\"\"\n",
    "        This method generates text of a given length. \n",
    "\n",
    "        text_length: int -- Length for the output text including `text_seed`.\n",
    "        text_seed: List[str] -- Given text to calculates probas for next words.\n",
    "        prob_method: str -- Specifies what method to use: with or without smoothing.\n",
    "\n",
    "        For example\n",
    "        >>> lm.generate(2)\n",
    "        [\"hello\", \"world\"]\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit(self, \n",
    "            sequence_of_tokens: Iterable[str]\n",
    "           ):\n",
    "        \"\"\"\n",
    "        This method learns probabilities based on given sequence of tokens and\n",
    "        updates `self.vocab`.\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "\n",
    "        For example\n",
    "        >>> lm.update(['hello', 'world'])\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @_check_fit  \n",
    "    def perplexity(self, \n",
    "                   sequence_of_tokens: Union[Iterable[str], Iterable[Tuple[str]]]\n",
    "                   ) -> float:\n",
    "        \"\"\"\n",
    "        This method returns perplexity for a given sequence of tokens\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_HW0_template_LM_class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
