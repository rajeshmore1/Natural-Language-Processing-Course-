# Module #1: Language Modeling

## 2.(Theory) - Introduction to Natural Language Processing
High-level overview of common NLP tasks: Essential NLP Guide; 
https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/
Linguistic knowledge in NLP: an article.
https://towardsdatascience.com/linguistic-knowledge-in-natural-language-processing-332630f43ce1

## 2.NLP data preprocessing:
Ultimative preprocessing pipeline with some necessary EDA: an article;
https://medium.com/analytics-vidhya/text-preprocessing-for-nlp-natural-language-processing-beginners-to-master-fd82dfecf95
Kaggle kernel with ultimative preprocessing pipeline: kaggle kernel;
https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing/notebook
Keras text preprocessing -- Tokenizer and pad_sequences: an article.
https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html

## 3. (Theory) - Character and word n-gram Language Models:
Standford NLP: N-Grams language Models: video guide;
https://archive.org/details/41IntroductionToNGramsStanfordNLPProfessorDanJurafskyChrisManning/
N-Grams Language Models and smoothing:  a chapter from "Speech and Language Processing" by D. Jurafsky and  J.H. Martin;
https://web.stanford.edu/~jurafsky/slp3/3.pdf
Another explanation of LM: an article;
https://mchromiak.github.io/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/#.Y1EH3HZBw2x
Perplexity in LM: an article.
https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3

## 4. (Theory) - Vector Semantics and NN models:
Vector Semantics: a chapter from "Speech and Language Processing" by D. Jurafsky and  J.H. Martin - contains description of concept vector space in the field of NLP, first touch to embeddings;
https://web.stanford.edu/~jurafsky/slp3/6.pdf
Neural Language Model: a chapter from "Speech and Language Processing" by D. Jurafsky and  J.H. Martin.
https://web.stanford.edu/~jurafsky/slp3/7.pdf

## 5. Practical guides for LM in Keras/PyTorch:
Keras Embedding Layer explanation: kaggle kernel;
https://www.kaggle.com/code/rajmehra03/a-detailed-explanation-of-keras-embedding-layer/notebook
Word-based Language Model: articles one and two;
https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/
https://keras.io/examples/generative/text_generation_with_miniature_gpt/
Character-based Language Model: articles one and two;
https://towardsdatascience.com/simple-character-based-neural-language-model-for-poem-generator-using-keras-8295f52ff5c2
https://keras.io/examples/generative/lstm_character_level_text_generation/
PyTorch Word-level LM: example;
https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/
PyTorch Character-level LM: example.
https://www.kaggle.com/code/francescapaulin/character-level-lstm-in-pytorch/notebook

## 6. Useful NLP tools:
NLTK Library: guide;
https://www.nltk.org/book/
Language Models in NLTK: API, tutorial.
https://www.nltk.org/api/nltk.lm.html
https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook
Spacy tutorial: guide with preprocessing and practical tasks;
https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%e2%80%8bin-python/
Russian language model for Spacy: github project;
https://github.com/buriy/spacy-ru
Regular Expressions applied to NLP: a chapter from "Speech and Language Processing" by D. Jurafsky and  J.H. Martin. Starting from 2.2.
https://web.stanford.edu/~jurafsky/slp3/2.pdf

## 7. Useful tools for preprocessing of Russian words:
Pymorphy: documentation;
https://pymorphy2.readthedocs.io/en/latest/user/guide.html
Pymystem: github project;
https://github.com/nlpub/pymystem3
Natasha Project: github project (see Libraries section).
https://natasha.github.io/

## 8. End-to-end NLP task tutorial:
An article (contains some techniques covered in further sections of this course).
https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/

## 9. How to train transformer (BERT, ELECTRA, etc.) model for language generation: Colab notebook

https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=M1oqh0F6W3ad

## 10.Useful articles and videos:

An article about Recurrent Neural Networks: link;
http://karpathy.github.io/2015/05/21/rnn-effectiveness/
KenLM: github proj.
https://github.com/kpu/kenlm

